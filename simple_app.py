import streamlit as st
import pandas as pd
import numpy as np
import shap

# Import joblib
try:
    import joblib
    JOBLIB_AVAILABLE = True
except:
    JOBLIB_AVAILABLE = False
    st.warning("âš ï¸ joblib missing â€” demo mode enabled")

# Streamlit settings
st.set_page_config(page_title="Fair AI Demo", page_icon="ðŸ¤–")
st.title("ðŸŽ¯ Fair AI Income Predictor")
st.write("Fair AI model with Explainability (SHAP).")

# Load models
model = scaler = explainer = None

if JOBLIB_AVAILABLE:
    try:
        model = joblib.load("models/baseline_model.pkl")
        scaler = joblib.load("models/scaler.pkl")
        explainer = joblib.load("models/explainer.pkl")
        st.success("âœ… Models + Explainer loaded")
    except Exception as e:
        st.error(f"âŒ Error loading models: {e}")
else:
    st.warning("âš ï¸ joblib not installed â€” running demo rules.")

# Inputs
st.subheader("Enter Details")
age = st.slider("Age", 18, 80, 35)
education = st.slider("Education Level", 1, 16, 13)
hours = st.slider("Hours per Week", 10, 80, 40)
gender = st.selectbox("Gender", ["Female", "Male"])
race = st.selectbox("Race", ["Non-White", "White"])

gender_num = 1 if gender == "Male" else 0
race_num = 1 if race == "White" else 0

if st.button("Predict Income"):
    features = np.array([[age, education, hours, gender_num, race_num]])

    if model:
        try:
            features_scaled = scaler.transform(features)
            prediction = model.predict(features_scaled)[0]
            probability = model.predict_proba(features_scaled)[0][1]
        except:
            prediction = int(age > 30 and education > 12)
            probability = 0.8 if prediction else 0.3
    else:
        prediction = int(age > 30 and education > 12)
        probability = 0.8 if prediction else 0.3

    # Output
    st.subheader("ðŸ” Prediction Result")
    st.metric("Confidence", f"{probability:.1%}")
    if prediction:
        st.success("HIGH INCOME (>50K)")
    else:
        st.info("LOW INCOME (<=50K)")

    # Fairness check
    if gender == "Female" and probability < 0.4:
        st.warning("âš ï¸ Possible gender bias detected")
# -------------------------------
# EXPLAINABILITY (SHAP)
# -------------------------------
import shap

st.subheader("ðŸ§  Explainability (SHAP)")

try:
    # Only run SHAP if real model available
    if model is not None and JOBLIB_AVAILABLE:
        explainer = shap.TreeExplainer(model)

        # SHAP requires 2D array
        shap_values = explainer.shap_values(features_scaled)

        # Convert SHAP array to 1D for display
        if isinstance(shap_values, list):  
            shap_values = shap_values[1]  # for binary classifier

        shap_values = shap_values.reshape(-1)

        feature_names = ["Age", "Education", "Hours", "Gender", "Race"]
        shap_df = pd.DataFrame({
            "Feature": feature_names,
            "Importance": shap_values
        }).sort_values("Importance", key=abs, ascending=False)

        st.write("### ðŸ” Feature Importance for this Prediction:")
        st.dataframe(shap_df)

    else:
        st.warning("Explainability not available in demo mode.")

except Exception as e:
    st.error(f"Explainability unavailable: {str(e)}")


    # ---------------------------------
    # SHAP EXPLAINABILITY SECTION
    # ---------------------------------
    st.subheader("ðŸ§  Explainability (SHAP Feature Importance)")

    if explainer:
        try:
            shap_values = explainer.shap_values(features_scaled)[1]

            shap_df = pd.DataFrame({
                "Feature": ["Age", "Education", "Hours", "Gender", "Race"],
                "SHAP Value": shap_values[0]
            })

            st.table(shap_df)
            st.caption("Higher SHAP values â†’ stronger contribution to HIGH income.")
        except Exception as e:
            st.error(f"SHAP Error: {e}")
    else:
        st.info("SHAP explainer not available.")

st.markdown("---")
st.write("Built with â¤ï¸ using Streamlit")
